{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"a3_written.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPpd0CTqQn7BxtOjjYAB3w1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# CS 224n Assignment #3: Dependency Parsing \n","In this assignment, you will build a neural dependency parser using PyTorch. For a review of the fundamentals of PyTorch, please check out the PyTorch review session on Canvas. In Part 1, you will learn about two general neural network techniques (Adam Optimization and Dropout). In Part 2, you will implement and\n","train a dependency parser using the techniques from Part 1, before analyzing a few erroneous dependency parses."],"metadata":{"id":"qHC3xrjEDd9l"}},{"cell_type":"markdown","source":["## 1. Machine Learning & Neural Networks (8 points) "],"metadata":{"id":"LUzNp6BzHK_V"}},{"cell_type":"markdown","source":["- (a) (4 points) Adam Optimizer  \n","Recall the standard Stochastic Gradient Descent update rule:  \n","$$ğœ½ \\leftarrow ğœ½ - \\alpha\\nabla_ğœ½ J_{\\text{minibatch}}(ğœ½)$$  \n","where ğœ½ is a vector containing all of the model parameters, $J$ is the loss function, $\\nabla_ğœ½ J_{\\text{minibatch}}(ğœ½)$ is the gradient of the loss function with respect to the parameters on a minibatch of data, and $\\alpha$ is the learning rate. Adam Optimization$^1$ uses a more sophisticated update rule with two additional steps.$^2$  \n","  - i. (2 points) First, Adam uses a trick called *momentum* by keeping track of ğ¦, a rolling average of the gradients:  \n","  $$\\begin{align*}\n","  ğ¦ &\\leftarrow \\beta_1 ğ¦ + (1-\\beta_1)\\nabla_ğœ½ J_{\\text{minibatch}}(ğœ½) \\\\\n","  ğœ½ &\\leftarrow ğœ½ - \\alpha ğ¦\n","  \\end{align*}$$  \n","  where $\\beta_1$ 1 is a hyperparameter between 0 and 1 (often set to 0.9). Briefly explain in 2-4 sentences (you donâ€™t need to prove mathematically, just give an intuition) how using ğ¦ stops the updates from varying as much and why this low variance may be helpful to learning, overall.  \n","  $\\color{red}{*Answer*}$  \n","By using momentum, the effect of the current gradient at each step is multiplied by $(1-\\beta_1)$ and the parameter updates continue to move in the direction as the previous iterations. This method can help avoid local minima. Also, the model can be robust to noisy gradients from minibatches.  \n","  <br/>  \n","  - ii. (2 points) Adam extends the idea of *momentum* with the trick of *adaptive learning rates* by keeping track of ğ¯, a rolling average of the magnitudes of the gradients:  \n","  $$\\begin{align*}\n","  ğ¦ &\\leftarrow \\beta_1 ğ¦ + (1-\\beta_1)\\nabla_ğœ½ J_{\\text{minibatch}}(ğœ½) \\\\\n","  ğ¯ &\\leftarrow \\beta_2 ğ¯ + (1-\\beta_2)(\\nabla_ğœ½ J_{\\text{minibatch}}(ğœ½) \\odot \\nabla_ğœ½ J_{\\text{minibatch}}(ğœ½)) \\\\\n","  ğœ½ &\\leftarrow ğœ½ - \\alpha ğ¦ / \\sqrt{ğ¯}\n","  \\end{align*}$$  \n","  where âŠ™ and $/$ denote elementwise multiplication and division (so ğ³âŠ™ğ³ is elementwise squaring) and $\\beta_2$ is a hyperparameter between 0 and 1 (often set to 0.99). Since Adam divides the update\n","  by $\\sqrt{ğ¯}$, which of the model parameters will get larger updates? Why might this help with\n","learning?  \n","  $\\color{red}{*Answer*}$  \n","When the gradient at the current step is large, the value of $\\sqrt{ğ¯}$ gets bigger and the parameters of those will be regularized to have small updates. On the other hand, if the current gradient is very small(<1), the update step will get larger. Then the model can escape a saddle point much faster."],"metadata":{"id":"xhvq5_PFDd4K"}},{"cell_type":"markdown","source":["- (b) (4 points) Dropout$^3$ is a regularization technique. During training, dropout randomly sets units in the hidden layer ğ¡ to zero with probability $p_{\\text{drop}}$ (dropping different units each minibatch), and then multiplies ğ¡ by a constant $\\gamma$. We can write this as:  \n","$$ğ¡_{\\text{drop}} = \\gamma ğ \\odot ğ¡$$  \n","where $ğ\\in \\{ 0,1\\}^{D_h}$ ($D_h$ is the size of ğ¡) is a mask vector where each entry is 0 with probability $p_{\\text{drop}}$ and 1 with probability (1-$p_{\\text{drop}}$). $\\gamma$ is chosen such that the expected value of $ğ¡_{\\text{drop}}$ is ğ¡:  \n","$$\\mathbb{E}_{p_{\\text{drop}}} \\left[ ğ¡_{\\text{drop}}\\right]_i = h_i$$  \n","for all $i\\in \\{ 1,\\ldots, D_h\\}$.  \n","  - i. (2 points) What must $\\gamma$ equal in terms of $p_{\\text{drop}}$? Briefly justify your answer or show your math derivation using the equations given above.  \n","  $\\color{red}{*Answer*}$  \n","  $$\\begin{align*}\\mathbb{E}_{p_{\\text{drop}}} \\left[ ğ¡_{\\text{drop}}\\right]_i &= \\mathbb{E}_{p_{\\text{drop}}}\\left[ \\gamma d_i \\times h_i\\right] \\\\\n","  &= \\gamma \\mathbb{E}_{p_{\\text{drop}}}\\left[ d_i \\times h_i\\right]\\\\\n","  &= \\gamma \\left[ p_{\\text{drop}}\\cdot 0 + (1-p_{\\text{drop}}) \\cdot h_i \\right] \\\\\n","  &= \\gamma (1-p_{\\text{drop}}) h_i = h_i \\\\\n","  &\\therefore \\gamma = \\frac{1}{(1-p_{\\text{drop}})}\n","  \\end{align*}$$  \n","  <br/>\n","  - ii. (2 points) Why should dropout be applied during training? Why should dropout __NOT__ be applied during evaluation? (Hint: it may help to look at the paper linked above in the write-up.)  \n","  $\\color{red}{*Answer*}$  \n","  We use dropout to prevent overfitting in training step. If we apply dropout in test time, some neurons dropped and the learned weights of those will not contribute to evalutation."],"metadata":{"id":"lrW88ZGu6XUq"}},{"cell_type":"markdown","source":["## 2. Neural Transition-Based Dependency Parsing (44 points)  \n","In this section, youâ€™ll be implementing a neural-network based dependency parser with the goal of maximizing performance on the UAS (Unlabeled Attachment Score) metric.  \n","\n","Before you begin, please follow the README to install all the needed dependencies for the assignment. We will be using PyTorch 1.7.1 from <https://pytorch.org/get-started/locally/> with the `CUDA` option set to `None`, and the tqdm package â€“ which produces progress bar visualizations throughout your training process. The official PyTorch website is a great resource that includes tutorials for understanding PyTorchâ€™s Tensor library and neural networks.  \n","\n","A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between *head* words, and words which modify those heads. There are multiple types of dependency parsers, including transition-based parsers, graph-based parsers, and feature-based parsers. Your implementation will be a transition-based parser, which incrementally builds up a parse one step at a time. At every step it maintains a *partial* parse, which is represented as follows:  \n","- A *stack* of words that are currently being processed.\n","- A *buffer* of words yet to be processed.\n","- A list of *dependencies* predicted by the parser.  \n","\n","Initially, the stack only contains ROOT, the dependencies list is empty, and the buffer contains all words of the sentence in order. At each step, the parser applies a *transition* to the partial parse until its buffer is empty and the stack size is 1. The following transitions can be applied:  \n","- `SHIFT`: removes the first word from the buffer and pushes it onto the stack  \n","- `LEFT-ARC`: marks the second (second most recently added) item on the stack as a dependent of the first item and removes the second item from the stack, adding a *first_word* â†’ *second_word* dependency to the dependency list.\n","- `RIGHT-ARC`: marks the first (most recently added) item on the stack as a dependent of the second item and removes the first item from the stack, adding a *second_word* â†’ *first_word* dependency to the dependency list.  \n","  \n","On each step, your parser will decide among the three transitions using a neural network classifier.  \n","- (a) (4 points) Go through the sequence of transitions needed for parsing the sentence _\"I parsed this sentence correctly\"_. The dependency tree for the sentence is shown below. At each step, give the configuration of the stack and buffer, as well as what transition was applied this step and what new dependency was added (if any). The first three steps are provided below as an example.  \n","  $\\color{red}{*Answer*}$  \n","\n","| Stack | Buffer | New dependency | Transition |\n","| :--------- | :--------- | :--------- | :--------- | \n","| [ROOT] | [I, parsed, this, sentence, correctly] | | Initial Configuration |\n","| [ROOT, I] | [parsed, this, sentence, correctly] | | `SHIFT` |\n","| [ROOT, I, parsed] | [this, sentence, correctly] | | `SHIFT` |\n","| [ROOT, parsed] | [this, sentence, correctly] | parsed $\\rightarrow$ I | `LEFT-ARC` |\n","| [ROOT, parsed, this] | [sentence, correctly] | | `SHIFT` |\n","| [ROOT, parsed, this, sentence] | [correctly] | | `SHIFT` |\n","| [ROOT, parsed, sentence] | [correctly] | sentence $\\rightarrow$ this | `LEFT-ARC` |\n","| [ROOT, parsed] | [correctly] | parsed $\\rightarrow$ sentence | `RIGHT-ARC` |\n","| [ROOT, parsed, correctly] | [] | | `SHIFT` |\n","| [ROOT, parsed] | [] | parsed $\\rightarrow$ correctly | `RIGHT-ARC` |\n","| [ROOT] | [] | ROOT $\\rightarrow$ parsed | `RIGHT-ARC` |\n","  <br/>\n","\n","- (b) (2 points) A sentence containing *n* words will be parsed in how many steps (in terms of *n*)? Briefly explain in 1-2 sentences why.  \n","\n","  $\\color{red}{*Answer*}$  \n","  *2n*: In processing, each word should be added to *buffer* and removed.  \n","  <br/>\n","- (c), (d), (e)  \n","  __Coding__: Implement `parser_transitions.py`,  `parser_model.py`, `run.py`  \n","\n","  $\\color{red}{*Result*}$\n","  - Average Train Loss: 0.0674  \n","  - best dev UAS: 88.81  \n","  - test UAS: 89.39  \n","  <br/>\n","- (f) (12 points) For each sentence, state the type of error, the incorrect dependency, and the correct dependency. While each sentence should have a unique error type, there may be multiple possible correct dependencies for some of the sentences.  \n","  Here are four types of parsing error:  \n","  - __Prepositional Phrase Attachment Error__: a prepositional phrase is attached to the wrong head word.  \n","  - __Verb Phrase Attachment Error__: a verb phrase is attached to the wrong head word.  \n","  - __Modifier Attachment Error__: a modifier is attached to the wrong head word.  \n","  - __Coordination Attachment Error__: the second conjunct is attached to the wrong head word.  \n","\n","  $\\color{red}{*Answer*}$  \n","  - i. _\"I disembarked and was heading to a wedding fearing my death.\"_  \n","    - __Error type__: __Verb Phrase Attachment Error__  \n","    - __Incorrect dependency__: *wedding* â†’ *fearing*\n","    - __Correct dependency__: *heading* â†’ *fearing*\n","  - ii. _\"It makes me want to rush out and rescue people from dilemmas of their own making.\"_  \n","    - __Error type__: __Coordination Attachment Error__ \n","    - __Incorrect dependency__: *makes* â†’ *rescue*\n","    - __Correct dependency__: *rush* â†’ *rescue*\n","  - iii. _\"It is on loan from a guy named Joe Oâ€™Neill in Midland, Texas.\"_\n","    - __Error type__: __Prepositional Phrase Attachment Error__\n","    - __Incorrect dependency__: *named* â†’ *Midland*\n","    - __Correct dependency__: *guy* â†’ *Midland*\n","  - iv. _\"Brian has been one of the most crucial elements to the success of Mozilla software.\"_\n","    - __Error type__: __Modifier Attachment Error__\n","    - __Incorrect dependency__: *elements* â†’ *most*\n","    - __Correct dependency__: *crucial* â†’ *most*\n"],"metadata":{"id":"_dqRllrm6Z_E"}}]}